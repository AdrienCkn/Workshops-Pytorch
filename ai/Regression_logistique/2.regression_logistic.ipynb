{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Mindset of a neural network with logistic regression\n",
                "\n",
                "In this workshop, you will code a neural network which can recognize cats from an image. You will learn the mindset of how a neural network works and, in general, get an idea of what really is deep learning.\n",
                "\n",
                "**Instructions:**\n",
                "- Don't use for or while loops unless explicitly asked.\n",
                "\n",
                "**You will learn to:**\n",
                "- Create the general architecture of a learning model including:\n",
                "  - The initialization of parameters\n",
                "  - The computation of the cost function and its gradient\n",
                "  - The usage of an optimization algorithm\n",
                "- Regroup the three above functions for the model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1 - Packages ##\n",
                "\n",
                "Let's begin by importing the required packages:\n",
                "- [numpy](www.numpy.org) is the fundamental package for scientific calculations in python\n",
                "- [h5py](http://www.h5py.org) is a package allowing you to interact with a dataset stored in a H5 file\n",
                "- [matplotlib](http://matplotlib.org) is a popular library for displaying graphs in python\n",
                "- [PIL](http://www.pythonware.com/products/pil/) and [scipy](https://www.scipy.org/) are used here to test the model with your own photos at the end"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import h5py\n",
                "import scipy\n",
                "from PIL import Image\n",
                "from scipy import ndimage\n",
                "from lr_utils import load_dataset\n",
                "\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": [
                "## 2 - Overview of the problem##\n",
                "\n",
                "**Problem**:\n",
                "We gave you a dataset (\"data.h5\") containing:\n",
                "     - a training set of m_train images, each labeled as being a cat (y=1) or non-cat (y=0)\n",
                "     - a test set of m_test images, each labeled as cat or non-cat\n",
                "     - each image is of the form (num_px, num_px, 3) where 3 corresponds to the three RGB channels. Each image is therefore square with side num_px.\n",
                "\n",
                "You will build a simple image recognition algorithm that can correctly classify other people's cat images.\n",
                "\n",
                "Let's explore our dataset first. Let's start by importing it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data loading\n",
                "x_train_orig, y_train, x_test_orig, y_test, classes = load_dataset()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We added \"_orig\" at the end of the image datasets (train and test) because we will process them later. In general, when you are given a data set, it is never perfect. You will still need to go through a `cleanup` stage called `preprocessing`. After this step you will end up with x_train and x_test (y_train and y_test don't need preprocessing)\n",
                "\n",
                "Each row of your x_train_orig and y_test_orig is an array representing an image. You can view one by running the following code. You can also change the value of `index` if you want to see other images."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example of an image\n",
                "index = 30\n",
                "plt.imshow(x_train_orig[index])\n",
                "if classes[np.squeeze(y_train[:, index])] == b'cat':\n",
                "    print(\"y = \" + str(y_train[:, index]) + \", this is a cat photo\")\n",
                "else:\n",
                "    print(\"y = \" + str(y_train[:, index]) + \", this is not a cat photo\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Many bugs in deep learning come from incorrect matrix/vector dimensions. By paying attention to it, you will avoid many bugs.\n",
                "\n",
                "\n",
                "**Exercise:** Find the correct values of:\n",
                "     - m_train (number of train images)\n",
                "     - m_test (number of test images)\n",
                "     - num_px (length/width of an image)\n",
                "    \n",
                "Remember that x_train_orig is a numpy array of the form (m_train, num_px, num_px, 3). For example, you can access `m_train` by writing `x_train_orig.shape[0]`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "### Start of code ### (≈ 3 lines of code)\n",
                "m_train = None\n",
                "m_test = None\n",
                "num_px = None\n",
                "### End of code ###\n",
                "\n",
                "print (\"Train size: m_train = \" + str(m_train))\n",
                "print (\"Test size: m_test = \" + str(m_test))\n",
                "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
                "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
                "print (\"x_train shape: \" + str(x_train_orig.shape))\n",
                "print (\"y_train shape: \" + str(y_train.shape))\n",
                "print (\"x_test shape: \" + str(x_test_orig.shape))\n",
                "print (\"y_test shape: \" + str(y_test.shape))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Expected result for m_train, m_test and num_px**: \n",
                "\n",
                "<table style=\"width:15%\">\n",
                "  <tr>\n",
                "    <td>m_train</td>\n",
                "    <td> 209 </td> \n",
                "  </tr>\n",
                "  \n",
                "  <tr>\n",
                "    <td>m_test</td>\n",
                "    <td> 50 </td> \n",
                "  </tr>\n",
                "  \n",
                "  <tr>\n",
                "    <td>num_px</td>\n",
                "    <td> 64 </td> \n",
                "  </tr>\n",
                "  \n",
                "</table>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For convenience, you should transform your images of the shape (num_px, num_px, 3) into a numpy array of the shape (num_px $*$ num_px $*$ 3, 1). After that, our datasets (train and test) will be numpy arrays in which each column represents a flattened image. There should be m_train and m_test columns.\n",
                "\n",
                "\n",
                "**Exercise:**\n",
                "Transform the train and test datasets so that images of the shape (num_px, num_px, 3) are flattened into simple vectors of the shape (num\\_px $*$ num\\_px $*$ 3, 1).\n",
                "\n",
                "Small trick: when you want to flatten an X matrix of the form (a,b,c,d) into an X_flatten matrix of the form (b$*$c$*$d, a):\n",
                "```python\n",
                "X_flatten = X.reshape(X.shape[0], -1).T # X.T is the transpose of X\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "### Start of code ### (≈ 2 lines of code)\n",
                "x_train_flatten = None\n",
                "x_test_flatten = None\n",
                "### End of code ###\n",
                "\n",
                "print (\"x_train_flatten shape: \" + str(x_train_flatten.shape))\n",
                "print (\"y_train shape: \" + str(y_train.shape))\n",
                "print (\"x_test_flatten shape: \" + str(x_test_flatten.shape))\n",
                "print (\"y_test shape: \" + str(y_test.shape))\n",
                "print (\"check 1 random after reshaping: \" + str(x_train_flatten[5:10,1]))\n",
                "print (\"check 2 random after reshaping: \" + str(x_train_flatten[17:22,34]))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Expected result**: \n",
                "\n",
                "    x_train_flatten shape: (12288, 209)\n",
                "    y_train shape: (1, 209)\n",
                "    x_test_flatten shape: (12288, 50)\n",
                "    y_test shape: (1, 50)\n",
                "    check 1 random after reshaping: [182 188 179 174 213]\n",
                "    check 2 random after reshaping: [20 16  3 22 15]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To represent color images, red, green and blue (RGB) channels must be specified for each pixel, and the value of each pixel is actually a vector of 3 numbers between 0 and 255.\n",
                "\n",
                "A fairly common preprocessing step in machine learning is to center and normalize your dataset, which means you'll calculate the average of the entire numpy array and then divide each instance in that dataset by the standard deviation. For images, it is simpler and more practical to only divide each row of the dataset by 255 (the maximum value of a color channel). You then end up with a numpy array comprising numbers between 0 and 1.\n",
                "\n",
                "Let's normalize our dataset:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x_train = x_train_flatten/255.\n",
                "x_test = x_test_flatten/255."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<font color='blue'>\n",
                "    \n",
                "**What you need to remember:**\n",
                "\n",
                "Common preprocessing steps:\n",
                "- Analyze data by displaying dataset dimensions and shapes (m_train, m_test, num_px, etc.)\n",
                "- Transform the datasets so that each example becomes a dimension vector (num_px \\* num_px \\* 3, 1)\n",
                "- Normalize data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3 - General architecture of the learning algorithm ##\n",
                "\n",
                "It's time to build a simple algorithm to recognize a cat from a non-cat from an image.\n",
                "\n",
                "You will build a logistic regression, while following the mindset (state of mind) of a neural network. The following figure explains why **logistic regression** is a very simple neural network!\n",
                "\n",
                "<img src=\"images/LogReg_kiank.png\" style=\"width:650px;height:400px;\">\n",
                "\n",
                "**Mathematical expression of the algorithm**:\n",
                "\n",
                "For each example $x^{(i)}$:\n",
                "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
                "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$\n",
                "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) = - y^{(i)} \\log(a^{(i)}) - (1-y^ {(i)} ) \\log(1-a^{(i)})\\tag{3}$$\n",
                "\n",
                "The cost is then calculated by adding all the **losses** of each example.\n",
                "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n",
                "\n",
                "**Key steps**:\n",
                "In this exercise, you will cover the following steps:\n",
                "    - Initialize model parameters\n",
                "    - Learn the parameters to the model while minimizing the cost\n",
                "    - Use learned parameters to make predictions (on the test dataset)\n",
                "    - Analyze the results and conclude\n",
                "\n",
                "Feel free to call one of the assistants if you need help."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4 - Build the different parts of the algorithm ##\n",
                "\n",
                "The main steps to build a neural network are:\n",
                "1. Define the structure of the model (such as the number of input features)\n",
                "2. Initialize model parameters\n",
                "3. Buckle:\n",
                "     - Calculate the current loss (forward spread)\n",
                "     - Calculate the current gradient (backward propagation)\n",
                "     - Update parameters (gradient descent)\n",
                "    \n",
                "### 4.1 - Useful functions\n",
                "\n",
                "**Exercise**:\n",
                "Using your code from the last workshop (on Python and numpy), implement the sigmoid function. As you saw in the previous figure, you need to calculate $sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$ to make predictions. Use np.exp()."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def sigmoid(z):\n",
                "    \"\"\"\n",
                "    Arguments:\n",
                "    z -- A scalar or a numpy array\n",
                "    \n",
                "    Return:\n",
                "    s -- sigmoid(z)\n",
                "    \"\"\"\n",
                "\n",
                "    ### Start of code ### (≈ 1 line of code)\n",
                "    s = None\n",
                "    ### End of code ###\n",
                "    \n",
                "    return s"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "print (\"sigmoid([-1, 0, 0.5, 2, 3]) = \" + str(sigmoid(np.array([-1, 0, 0.5, 2, 3]))))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Expected result**: \n",
                "\n",
                "sigmoid([-1, 0, 0.5, 2, 3]) = [0.26894142 0.5        0.62245933 0.88079708 0.95257413]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 - Parameter initialization\n",
                "\n",
                "**Exercise:**\n",
                "Implement parameter initialization in the next cell. You must initialize w as a vector containing only 0s. If you don't know which numpy function to use, check out np.zeros() in the numpy docs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def initialize(dim):\n",
                "    \"\"\"\n",
                "    This function creates a vector of shape zeros (dim, 1) for w and initializes b to 0\n",
                "\n",
                "    Argument:\n",
                "    dim -- size of the vector w we want (or the number of parameters in this case)\n",
                "\n",
                "    Returns:\n",
                "    w -- initialized shape vector (dim, 1)\n",
                "    b -- initialized scalar number corresponding to the bias\n",
                "    \"\"\"\n",
                "    \n",
                "    ### Start of code ### (≈ 2 lines of code)\n",
                "    w = None\n",
                "    b = None\n",
                "    ### End of code ###\n",
                "\n",
                "    assert(w.shape == (dim, 1))\n",
                "    assert(isinstance(b, float) or isinstance(b, int))\n",
                "    \n",
                "    return w, b"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dim = 5\n",
                "w, b = initialize(dim)\n",
                "print (\"w = \" + str(w))\n",
                "print (\"b = \" + str(b))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Expected result**: \n",
                "\n",
                "w = [[0.]\n",
                " [0.]\n",
                " [0.]\n",
                " [0.]\n",
                " [0.]]\n",
                " \n",
                "b = 0\n",
                "\n",
                "For images, w must be of shape (num_px $\\times$ num_px $\\times$ 3, 1)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.3 - Forward and Backward spread\n",
                "\n",
                "Now that your parameters are initialized, you can write forward and backward propagation steps to learn the parameters.\n",
                "\n",
                "\n",
                "**Exercise:** Implement the `propagate()` function which will calculate the cost function and its gradient.\n",
                "\n",
                "**Hints**:\n",
                "\n",
                "Forward Spread:\n",
                "- We give you X\n",
                "- You calculate $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{ (m)})$\n",
                "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+ (1-y^{(i)})\\log(1-a^{(i)})$\n",
                "\n",
                "Here are the two formulas you will use:\n",
                "\n",
                "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
                "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def propagation(w, b, X, Y):\n",
                "    \"\"\"\n",
                "    Arguments:\n",
                "    w -- the weights, a numpy array of size (num_px * num_px * 3, 1)\n",
                "    b -- the bias, a scalar number\n",
                "    X -- matrix of size (num_px * num_px * 3, number of examples)\n",
                "    Y -- the vector corresponding to labels (0 if non-cat, 1 if cat) of size (1, number of examples)\n",
                "\n",
                "    Return:\n",
                "    cost -- cost\n",
                "    dw -- loss gradient of the same shape as w\n",
                "    db -- loss gradient with same shape as b\n",
                "\n",
                "    Tips:\n",
                "    - Write your code step by step for propagation. np.log(), np.dot()\n",
                "    \"\"\"\n",
                "    \n",
                "    m = X.shape[1]\n",
                "\n",
                "    # Forward Propagation (from X to cost)\n",
                "    ### Start of code ### (≈ 2 lines of code)\n",
                "    # compute activation\n",
                "    A = None\n",
                "    # compute cost\n",
                "    cost = None\n",
                "    ### End of code ###\n",
                "    \n",
                "    # Backward Propagation (to find gradients)\n",
                "    ### Start of code ### (≈ 2 lines of code)\n",
                "    dw = None\n",
                "    db = None\n",
                "    ### End of code ###\n",
                "\n",
                "    assert(dw.shape == w.shape)\n",
                "    assert(db.dtype == float)\n",
                "    cost = np.squeeze(cost)\n",
                "    assert(cost.shape == ())\n",
                "    \n",
                "    grads = {\"dw\": dw,\n",
                "             \"db\": db}\n",
                "    \n",
                "    return grads, cost"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "w, b, X, Y = np.array([[4.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
                "grads, cost = propagation(w, b, X, Y)\n",
                "print (\"dw = \" + str(grads[\"dw\"]))\n",
                "print (\"db = \" + str(grads[\"db\"]))\n",
                "print (\"cost = \" + str(cost))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Expected result**:\n",
                "\n",
                "<table style=\"width:50%\">\n",
                "    <tr>\n",
                "        <td>  dw   </td>\n",
                "      <td> [[0.999923  ]\n",
                "         [2.39975403]]</td>\n",
                "    </tr>\n",
                "    <tr>\n",
                "        <td>  db  </td>\n",
                "        <td> 7.288578855054369e-05 </td>\n",
                "    </tr>\n",
                "    <tr>\n",
                "        <td>  cost  </td>\n",
                "        <td> 8.800077000774023 </td>\n",
                "    </tr>\n",
                "\n",
                "</table>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.4 - Optimization\n",
                "\n",
                "- You have initialized your settings.\n",
                "- You also know how to calculate a cost function and its gradient.\n",
                "- Now you want to update the parameters using gradient descent.\n",
                "\n",
                "\n",
                "**Exercise:** Write the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a given parameter $\\theta$, the update formula is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ corresponds to the **learning rate**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def optimization(w, b, X, Y, n_iterations, learning_rate, print_cost=False):\n",
                "    \"\"\"\n",
                "    Arguments:\n",
                "    w -- the weights, numpy array of size (num_px * num_px * 3, 1)\n",
                "    b -- the bias, a scalar\n",
                "    X -- matrix of size (num_px * num_px * 3, number of examples)\n",
                "    Y -- the vector corresponding to labels (0 if non-cat, 1 if cat) of size (1, number of examples)\n",
                "    n_iterations -- number of iterations in the optimization loop\n",
                "    learning_rate -- the learning rate\n",
                "    print_cost -- True to print the loss every 100 times\n",
                "    \n",
                "    Returns:\n",
                "    params -- python dictionary containing weights w and bias b\n",
                "    grads -- dictionary containing the gradients of w and b thanks to the cost function\n",
                "    costs -- list of all costs calculated during optimization, this will allow us to display the learning curve\n",
                "\n",
                "    Tips:\n",
                "    You will need to write 2 steps and iterate through them:\n",
                "        1) Calculate the cost and the gradient of the parameters. Use spread()\n",
                "        2) Update the parameters using gradient descent for w and b\n",
                "    \n",
                "    \"\"\"\n",
                "    \n",
                "    costs = []\n",
                "    \n",
                "    for i in range(n_iterations):\n",
                "        \n",
                "        \n",
                "        # Computing cost and gradients (≈ 1-4 lines of code)\n",
                "        ### Start of code ### \n",
                "        grads, cost = None\n",
                "        ### End of code ###\n",
                "        \n",
                "        # Retrieve derivates from grads\n",
                "        dw = grads[\"dw\"]\n",
                "        db = grads[\"db\"]\n",
                "        \n",
                "        # update (≈ 2 lines of code)\n",
                "        ### Start of code ###\n",
                "        w = None\n",
                "        b = None\n",
                "        ### End of code ###\n",
                "        \n",
                "        # Store the costs\n",
                "        if i % 100 == 0:\n",
                "            costs.append(cost)\n",
                "        \n",
                "        # Display cost every 100 iterations\n",
                "        if print_cost and i % 100 == 0:\n",
                "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
                "    \n",
                "    params = {\"w\": w,\n",
                "              \"b\": b}\n",
                "    \n",
                "    grads = {\"dw\": dw,\n",
                "             \"db\": db}\n",
                "    \n",
                "    return params, grads, costs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "params, grads, costs = optimization(w, b, X, Y, n_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
                "\n",
                "print (\"w = \" + str(params[\"w\"]))\n",
                "print (\"b = \" + str(params[\"b\"]))\n",
                "print (\"dw = \" + str(grads[\"dw\"]))\n",
                "print (\"db = \" + str(grads[\"db\"]))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Expected result**: \n",
                "\n",
                "w = [[ 3.11567021]\n",
                " [-0.10995646]]\n",
                "\n",
                "b = 1.9850788219795317\n",
                "\n",
                "dw = [[0.89699851]\n",
                " [2.07122651]]\n",
                "\n",
                "db = 0.09736680112994214"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Exercise:** The previous function displayed the learned weights w and bias b. They can be used to predict the labels of an X dataset. Implement the `prediction()` function. There are 2 steps to calculate the predictions:\n",
                "\n",
                "1. Calculate $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
                "\n",
                "2. Convert the inputs of A to 0 (if activation <= 0.5) or 1 (if activation > 0.5) and store them in a `Y_prediction` vector."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def prediction(w, b, X):\n",
                "    '''\n",
                "    Arguments:\n",
                "    w -- the weights, numpy array of size (num_px * num_px * 3, 1)\n",
                "    b -- the bias, a scalar\n",
                "    X -- matrix of size (num_px * num_px * 3, number of examples)\n",
                "\n",
                "    Returns:\n",
                "Y_prediction -- a numpy array (vector) containing all the predictions (0/1) of the examples of X\n",
                "    '''\n",
                "    \n",
                "    m = X.shape[1]\n",
                "    Y_prediction = np.zeros((1,m))\n",
                "    w = w.reshape(X.shape[0], 1)\n",
                "    \n",
                "    # Calculate the vector \"A\" corresponding to the predictions if a cat is present in the image\n",
                "    ### Start of code ### (≈ 1 line of code)\n",
                "    A = None\n",
                "    ### End of code ###\n",
                "    \n",
                "    for i in range(A.shape[1]):\n",
                "        \n",
                "        # Convert the predictions A[0,i] en p[0,i]\n",
                "        ### Start of code ### (≈ 4 lines of code)\n",
                "\n",
                "        ### End of code ###\n",
                "    \n",
                "    assert(Y_prediction.shape == (1, m))\n",
                "    \n",
                "    return Y_prediction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "w = np.array([[0.1124579],[0.23106775]])\n",
                "b = -0.3\n",
                "X = np.array([[1.,-1.2,-2.4],[1.4,2.5,0.6]])\n",
                "print (\"predictions = \" + str(prediction(w, b, X)))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Expected result**: \n",
                "\n",
                "<table style=\"width:30%\">\n",
                "    <tr>\n",
                "         <td>\n",
                "             predictions\n",
                "         </td>\n",
                "          <td>\n",
                "            [[ 1.  1.  0.]]\n",
                "         </td>  \n",
                "   </tr>\n",
                "\n",
                "</table>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": [
                "<font color='blue'>\n",
                "    \n",
                "**What you need to remember:**\n",
                "\n",
                "You have implemented several functions that:\n",
                "- initialize (w, b)\n",
                "- optimize the loss iteratively to learn the parameters (w, b):\n",
                "     - calculation of the cost and its gradient\n",
                "     - update parameters using gradient descent\n",
                "- uses the learned (w, b) to predict the labels of an example dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5 - Merge all functions into a single model ##\n",
                "\n",
                "You will now see how the overall model is structured by integrating all the blocks (the functions you have implemented) together, in the correct order.\n",
                "\n",
                "**Exercise**: Implement the \"model\" function. Use the following notations:\n",
                "     - Y_pred_test for your predictions on the test dataset\n",
                "     - Y_pred_train for your predictions on the tain dataset\n",
                "     - w, costs, grads for optimization() results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def model(X_train, Y_train, X_test, Y_test, n_iterations = 2000, learning_rate = 0.5, print_cost=False):\n",
                "    \"\"\" \n",
                "     Arguments:\n",
                "     X_train -- train dataset represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
                "     Y_train -- train data labels represented by a numpy array (vector) of shape (1, m_train)\n",
                "     X_test -- test dataset represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
                "     Y_test -- test data labels represented by a numpy array (vector) of shape (1, m_test)\n",
                "     n_iterations -- number of iterations in the optimization loop\n",
                "     learning_rate -- the learning rate\n",
                "     print_cost -- True to print the loss every 100 times\n",
                "\n",
                "\n",
                "     Returns:\n",
                "     d -- python dictionary containing all model information.\n",
                "    \"\"\"\n",
                "    \n",
                "    ### Start of code ###\n",
                "\n",
                "    # Initialize parameters with 0s (≈ 1 line of code)\n",
                "    w, b = None\n",
                "\n",
                "    # Gradient descent (≈ 1 line of code)\n",
                "    parameters, grads, costs = None\n",
                "    \n",
                "    # Retrieve parameters\n",
                "    w = parameters[\"w\"]\n",
                "    b = parameters[\"b\"]\n",
                "    \n",
                "    # Predict both train and test examples (≈ 2 lines of code)\n",
                "    Y_pred_test = None\n",
                "    Y_pred_train =  None\n",
                "\n",
                "    ### End of code ###\n",
                "\n",
                "    # Display accuracies\n",
                "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_pred_train - Y_train)) * 100))\n",
                "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_pred_test - Y_test)) * 100))\n",
                "\n",
                "    \n",
                "    d = {\"costs\": costs,\n",
                "         \"Y_pred_test\": Y_pred_test, \n",
                "         \"Y_pred_train\" : Y_pred_train, \n",
                "         \"w\" : w, \n",
                "         \"b\" : b,\n",
                "         \"learning_rate\" : learning_rate,\n",
                "         \"num_iterations\": n_iterations}\n",
                "    \n",
                "    return d"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Run the following cell to train your model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "d = model(x_train, y_train, x_test, y_test, n_iterations = 2000, learning_rate = 0.006, print_cost = True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Expected result**: \n",
                "\n",
                "Cost after iteration 0: 0.693147\n",
                "\n",
                "Cost after iteration 100: 0.649811\n",
                "\n",
                "Cost after iteration 200: 0.538312\n",
                "\n",
                "Cost after iteration 300: 0.439262\n",
                "\n",
                "Cost after iteration 400: 0.349825\n",
                "\n",
                "Cost after iteration 500: 0.278498\n",
                "\n",
                "Cost after iteration 600: 0.249764\n",
                "\n",
                "Cost after iteration 700: 0.231178\n",
                "\n",
                "Cost after iteration 800: 0.215229\n",
                "\n",
                "Cost after iteration 900: 0.201339\n",
                "\n",
                "Cost after iteration 1000: 0.189110\n",
                "\n",
                "Cost after iteration 1100: 0.178249\n",
                "\n",
                "Cost after iteration 1200: 0.168533\n",
                "\n",
                "Cost after iteration 1300: 0.159788\n",
                "\n",
                "Cost after iteration 1400: 0.151873\n",
                "\n",
                "Cost after iteration 1500: 0.144677\n",
                "\n",
                "Cost after iteration 1600: 0.138104\n",
                "\n",
                "Cost after iteration 1700: 0.132079\n",
                "\n",
                "Cost after iteration 1800: 0.126537\n",
                "\n",
                "Cost after iteration 1900: 0.121421\n",
                "\n",
                "train accuracy: 99.52153110047847 %\n",
                "\n",
                "test accuracy: 68.0 %\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": [
                "**Comment**: Your train accurary is very close to 100%. That's great, your model is functional and can recognize cats from the train dataset very well. However, the test accuracy is 68%. It's not too bad for the simple model we built given the small dataset provided, but don't worry, we'll build a better model in a future workshop.\n",
                "\n",
                "You also noticed that the model overinterpreted (overfit) the train data. We will see later methods to reduce overinterpretation (using regularization for example). Use the code below to display the cost function and its gradients."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot learning curve (with costs)\n",
                "costs = np.squeeze(d['costs'])\n",
                "plt.plot(costs)\n",
                "plt.ylabel('cost')\n",
                "plt.xlabel('iterations (par centaine)')\n",
                "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": [
                "**Interpretation**:\n",
                "You can see that the cost is decreasing. This shows that the parameters are being learned. However, you also see that the model is training too much on the train dataset. Try increasing the number of iterations in the cell above and re-run the cells. You should see that the accuracy of the training dataset increases but the test dataset decreases. This is called overfitting."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Some useful sources:\n",
                "- http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n",
                "- https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c"
            ]
        }
    ],
    "metadata": {
        "coursera": {
            "course_slug": "neural-networks-deep-learning",
            "graded_item_id": "XaIWT",
            "launcher_item_id": "zAgPl"
        },
        "kernelspec": {
            "display_name": "Python 3.10.6 64-bit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.6"
        },
        "vscode": {
            "interpreter": {
                "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
