{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python & Numpy basics\n",
    "\n",
    "This notebook will give you a brief introduction to the Python language. Even if you've already coded in Python before, it will be helpful to familiarize yourself with the methods you will need for this workshop.\n",
    "\n",
    "**Instructions:**\n",
    "- You will use Python 3\n",
    "- Don't use the 'for' or 'while' loops unless you are explicitly told to\n",
    "- After having coded your function, run the following cell in order to verify your result\n",
    "\n",
    "**At the end of this workshop, you will have learnt to:**\n",
    "- Use iPython notebooks\n",
    "- Use numpy methods for vector and matrix operations\n",
    "- Understand the concept of broadcasting\n",
    "- Vectorize code\n",
    "\n",
    "If you are stuck or don't understand a concept, don't hesitate to come ask for help. **That's why we're here !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About iPython Notebooks ##\n",
    "\n",
    "iPython notebooks are interactive code environments within a website. They allow you to execute code line by line and be able to directly visualize the content of variables or a result for example. You will use iPython notebooks for most of the workshops. You only need to write code between the `### Start of code ###` and `### End of code ####`. After having written your code, you can execute the cell either by pressing SHIFT+ENTER or by clicking 'Run' ( ▶️ button in the top bar ).\n",
    "\n",
    "You will often come across \"~ X lines of code\". They are simply estimations of how many lines you need: feel free to write more or less.\n",
    "\n",
    "**Exercise**: DeEnde the `test` variable as \"Hello world !\" in order to display \"Hello world !\" and execute that following two cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start of code ### (≈ 1 line of code)\n",
    "test = None\n",
    "### End of code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"test: \" + test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected result**:\n",
    "test: Hello world !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**What you should remember**:\n",
    "- Execute your cells by pressing SHIFT+ENTER (or by clicking Run)\n",
    "- Write your code only inside the intended cells\n",
    "- Don't modify the other cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Create basic functions with Numpy ##\n",
    "\n",
    "Numpy is the official package for scientific calculation in Python. It is maintained by a large community [](www.numpy.org).\n",
    "In this exercise, you will discover multiple key numpy methods such as `np.exp`, `np.log` and `np.reshape`. Remember them well because they will be useful for future workshops.\n",
    "\n",
    "### 1.1 - np.exp(), the sigmoid function ###\n",
    "\n",
    "Before using np.exp(), you will use math.exp() to implement the sigmoid function. This will help you understand why np.exp() is preferable to math.exp().\n",
    "\n",
    "**Exercise**: Create a method that returns the sigmoid of a real number x. Use math.exp() for the exponential function.\n",
    "\n",
    "**Rappel**:\n",
    "$sigmoid(x) = \\frac{1}{1+e^{-x}}$ is also known as the logistic function. It is a non linear function which isn't only used in Machine Learning (cf. logistic regression) but also in Deep Learning.\n",
    "\n",
    "<img src=\"images/Sigmoid.png\" style=\"width:500px;height:228px;\">\n",
    "\n",
    "In order to refer to a method belonging to a specific package, you can call it using `package_name.function()`. Execute the below code to see and example with `math.exp()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid_math(x):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- a scalar\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid_math(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Start of code ### (≈ 1 line of code)\n",
    "    s = None\n",
    "    ### End of code ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_math(2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected result**: \n",
    "0.9241418199787566"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `math` library is rarely used in deep learning because the method inputs are real numbers. In deep learning, we will mostly use matrices and vectors. That's where you can see the appeal for numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### One of the reasons we use \"numpy\" rather than \"math\" ###\n",
    "x = [1, 2, 3]\n",
    "sigmoid_math(x) # an error should pop up if you execute this cell\n",
    "                # That is because x is a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $ x = (x_1, x_2, ..., x_n)$ is a vector, then $np.exp(x)$ will apply the exponential function to every element of x. The result will therefore be: $np.exp(x) = (e^{x_1}, e^{x_2}, ..., e^{x_n})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# example of np.exp\n",
    "x = np.array([1, 2, 3])\n",
    "print(np.exp(x)) # the result is (exp(1), exp(2), exp(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, if x is a vector, then a Python operation like $s = x + 3$ ou $s = \\frac{1}{x}$ will return a vector of the same dimensions as x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a vector operation\n",
    "x = np.array([1, 2, 3])\n",
    "print (x + 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anytime you need additional information on a numpy function, we encourage you to check out [the official documentation](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.exp.html).\n",
    "\n",
    "You can also create new cell in this notebook and write `np.exp?` for example to get fast access to the docs.\n",
    "\n",
    "**Exercise**: Implement the sigmoid function using numpy.\n",
    "\n",
    "**Instructions**: x can be a real number, a vector or a matrix. The data structure that is used in numpy to represent these dimensions (vectors, matrices, etc.) are called numpy arrays. You don't need to know more for now.\n",
    "$$ \\text{For each x } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
    "    x_1  \\\\\n",
    "    x_2  \\\\\n",
    "    ...  \\\\\n",
    "    x_n  \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
    "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
    "    ...  \\\\\n",
    "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
    "\\end{pmatrix}\\tag{1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # np is an abbrevation of numpy. You will be able to write np.exp() instead of numpy.exp()\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- a scalar or a numpy array of any size\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Start of code ### (≈ 1 line of code)\n",
    "    s = None\n",
    "    ### End of code ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1.5, 2.5, 3.5])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected result**: \n",
    "array([0.81757448, 0.92414182, 0.97068777])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Sigmoid gradient\n",
    "\n",
    "As you heard at the start of the workshop, you will need to calculate the gradients in order to optimize the cost function by using backpropagation.\n",
    "\n",
    "**Exercise**: Implement the `sigmoid_gradient` function in order to compute the sigmoid function's gradient: $$dérivée\\_sigmoïde(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$\n",
    "You will code this function in 2 steps:\n",
    "1. Define s as the sigmoid of x. The sigmoid(x) method might be useful.\n",
    "2. Compute $\\sigma'(x) = s(1-s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_deriv(x):\n",
    "    \"\"\"    \n",
    "    Arguments:\n",
    "    x -- a scalar or a numpy array\n",
    "\n",
    "    Return:\n",
    "    ds -- your gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Start of code ### (≈ 2 lines of code)\n",
    "    s = None\n",
    "    ds = None\n",
    "    ### End of code ###\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1.5, 2.5, 3.5])\n",
    "print (\"sigmoid_deriv(x) = \" + str(sigmoid_deriv(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "sigmoid_deriv(x) = [0.14914645 0.07010372 0.02845302]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Reshaping Arrays ###\n",
    "\n",
    "Two numpy methods that are often used in deep learning are [np.shape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html) and [np.reshape()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html). \n",
    "- X.shape allows you to get the shape (dimensions) of a matrix or a vector called X.\n",
    "- X.reshape(...) is used to change X's shape\n",
    "\n",
    "\n",
    "\n",
    "For example, in cmoputer science, an image is represented by a 3 dimensional array $(height, width, depth = 3)$.\n",
    "However, when you use an image as an inout inside an algorithm, you need to convert it to a vector of shape (height\\*width\\*3, 1). In other words, you flatten or reshape the 3D array into a 1D array.\n",
    "\n",
    "\n",
    "<img src=\"images/image2vector_kiank.png\" style=\"width:500px;height:300;\">\n",
    "\n",
    "**Exercise**: Implement `image_to_vector()` which takes as input a matrix of shape (height, width, 3) and returns a vector of shape (longueur\\*largeur\\*3, 1). For example, if you want to reshape an array v of shape (a, b, c) into a vector of shape (a\\*b, c), you'll write:\n",
    "``` python\n",
    "v = v.reshape((v.shape[0]*v.shape[1], v.shape[2])) # v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c\n",
    "```\n",
    "- Please don't hardcode the image's dimensions as a constant ! Use `image.shape[0]`, etc to access the required values instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_vector(image):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    image -- a numpy array of shape (height, width, depth)\n",
    "    \n",
    "    Returns:\n",
    "    v -- un vecteur de shape (height*width*depth, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Start of code ### (≈ 1 line of code)\n",
    "    v = None\n",
    "    ### End of code ###\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An array of shape (3, 3, 2). In general, images will be of shape (nb_pix_x, nb_pix_y, 3) where depth corresponds to the three RGB values.\n",
    "image = np.array([[[ 0.67126139,  0.29381281],\n",
    "        [ 0.90714982,  0.52835547],\n",
    "        [ 0.42245251 ,  0.45012151]],\n",
    "\n",
    "       [[ 0.92814219,  0.96677647],\n",
    "        [ 0.85114703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "       [[ 0.6213595,  0.00531265],\n",
    "        [ 0.1210313,  0.49974237],\n",
    "        [ 0.3432129,  0.94631277]]])\n",
    "print (\"image_to_vector(image) = \" + str(image_to_vector(image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected result**: \n",
    "\n",
    "image_to_vector(image) = [[0.67126139]\n",
    " [0.29381281]\n",
    " [0.90714982]\n",
    " [0.52835547]\n",
    " [0.42245251]\n",
    " [0.45012151]\n",
    " [0.92814219]\n",
    " [0.96677647]\n",
    " [0.85114703]\n",
    " [0.52351845]\n",
    " [0.19981397]\n",
    " [0.27417313]\n",
    " [0.6213595 ]\n",
    " [0.00531265]\n",
    " [0.1210313 ]\n",
    " [0.49974237]\n",
    " [0.3432129 ]\n",
    " [0.94631277]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Normalize your data\n",
    "\n",
    "Another common technique that is used in Machine Learning and Deep Learning is data normalization. It often provides a better performance because the gradient descent will converge faster after normalization. Here, by normalization we mean replacing x by $ \\frac{x}{\\| x\\|} $ (dividing each vector of x by its norm). \n",
    "\n",
    "For example, if $$x = \n",
    "\\begin{bmatrix}\n",
    "    0 & 3 & 4 \\\\\n",
    "    2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}\\tag{3}$$ alors $$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix}\\tag{4} $$et        $$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}\\tag{5}$$ \n",
    "Note that you can divide matrices of different sizes and it works well thanks to broadcasting which we will talk about in section 1.5.\n",
    "\n",
    "\n",
    "**Exercise**: Implement normalize_rows() to normalize the lines of a matrix. After having applied this function to a matrix, each one of its lines must be a vector of unique size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_rows(x):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    x -- a numpy matrix x of shape (n, m)\n",
    "    \n",
    "    Returns:\n",
    "    x -- the normalized matrix x\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Start of code ### (≈ 2lines of code)\n",
    "    # First compute x's norm. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n",
    "    x_norm = None\n",
    "    \n",
    "    # Divide x by x_norm.\n",
    "    x = None\n",
    "    ### End of code ###\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [2, 3, 6],\n",
    "    [5, 2, 8]])\n",
    "print(\"normalize_rows(x) = \" + str(normalize_rows(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected result**: \n",
    "\n",
    "normalize_rows(x) = [[0.28571429 0.42857143 0.85714286]\n",
    " [0.51847585 0.20739034 0.82956136]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "In the normalize_rows function, you can try to print the shapes of x_norm and x and repeat the action as many times as you wish. You will observe that they can have different shapes. That's normal because x_norm takes the norm of each line of x. So x_norm has the same number of lines but only one column. How does this work when you divide x by x_norm ? That is what broadcasting is and we will learn more next !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 - Broadcasting and softmax ####\n",
    "\n",
    "Broadcasting is a very important concept to understand in numpy. It is a very useful notion to be able to make mathematical operations between arrays of different shapes. For more information, you can consult [the official broadcasting docs](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implement the softmax function by using numpy. You can see softmax as a normalization function used when your algorithm needs to classify 2 classes or more.\n",
    "\n",
    "Warning: Your code needs to be able to funtion with a vector but also a matrix\n",
    "\n",
    "**Instructions**:\n",
    "- $ \\text{For each } x \\in \\mathbb{R}^{1\\times n} \\text{,     } softmax(x) = softmax(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2 &&\n",
    "    ...  &&\n",
    "    x_n  \n",
    "\\end{bmatrix}) = \\begin{bmatrix}\n",
    "     \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix} $ \n",
    "\n",
    "- $\\text{For a matrix } x \\in \\mathbb{R}^{m \\times n} \\text{,  $x_{ij}$ corresponds to the element of the $i^{th}$ line and the $j^{th}$ column of $x$, we therefore have: }$  $$softmax(x) = softmax\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} = \\begin{pmatrix}\n",
    "    softmax\\text{(first line of x)}  \\\\\n",
    "    softmax\\text{(second line of x)} \\\\\n",
    "    ...  \\\\\n",
    "    softmax\\text{(last line of x)} \\\\\n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    x -- A vector or a numpy matrix of shape (n,n)\n",
    "\n",
    "    Returns:\n",
    "    s -- A numpy matrix equal to the softmax of x, of shape (n,n)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Start of code ### (≈ 3lines of code)\n",
    "    # Compute the exponential of each x element. Use np.exp()\n",
    "    x_exp = None\n",
    "\n",
    "    # Create an x_sum vector that contains the sum of each line of x_exp. Use np.sum(..., axis = 1, keepdims = True)\n",
    "    x_sum = None\n",
    "    \n",
    "    # Compute softmax(x) by dividing x_exp by x_sum. That will automatically use numpy's broadcasting.\n",
    "    s = x_exp / x_sum\n",
    "    ### End of code ###\n",
    "    print(x_exp.shape, x_sum.shape, s.shape)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vect = np.array([[9, 4, 0, 0 ,0]])\n",
    "\n",
    "print(\"softmax(x_vect) = \" + str(softmax(x_vect)))\n",
    "\n",
    "x_matr = np.array([\n",
    "    [1, 7, 5, 0, 6],\n",
    "    [3, 4, 0, 2 ,0]])\n",
    "\n",
    "print(\"softmax(x_matr) = \" + str(softmax(x_matr)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected result**:\n",
    "\n",
    "softmax(x_vect) = [[9.92941993e-01 6.69039052e-03 1.22538777e-04 1.22538777e-04\n",
    "  1.22538777e-04]]\n",
    "\n",
    "softmax(x_matr) = [[1.64525645e-03 6.63743823e-01 8.98279582e-02 6.05256022e-04\n",
    "  2.44177707e-01]\n",
    " [2.38906644e-01 6.49415590e-01 1.18944614e-02 8.78888428e-02\n",
    "  1.18944614e-02]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "- If you print the shapes of x_exp's, x_sum and s above this and rerun the cell, in the matrix' case you will see that x_sum is of shape (2, 1) while x_exp and s are of shape (2, 5). **x_exp/x_sum** works correctly thanks to broadcasting.\n",
    "\n",
    "Congratulations ! You have now acquired a good comprehension of python and its numpy library and have implemented some functions which will be helpful in deep learning !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**What you must remember:**\n",
    "- np.exp(x) works for any np.array and applies the exponential function to all its elements\n",
    "- the sigmoid function and its gradient\n",
    "- image_to_vector is often used in deep learning\n",
    "- np.reshape is often used. Later, you will see that its correct usage will prevent many bugs\n",
    "- numpy has very efficient build-in functions\n",
    "- broadcasting is extremely useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2) Vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En deep learning, on manipule de très gros datasets. De ce fait, une fonction de calcul non optimisée peut présenter un poids lourd à votre algorithme et produira un modèle qui mettrait des années à s'éxécuter. Pour être sûr que votre code est suffisamment optimisé en terme de calcul, vous utiliserez le concept de vectorisation. Par exemple, essayez de me dire quelle est la différence entre les différentes implémentations de produits dot/outer/elementwise.\n",
    "\n",
    "In deep learning, you manipulate many huge datasets. Therefore, a calculation function that isn't optimized can present a heavy toll on your algorithm and will produce a model which would takes years to run. To make sure your code is sufficiently optimized in terms of calculations, you shall use the concept of vectorisation. For example, try to tell me what the difference between the different implementations of dot/outer/elementwise products is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### Classic implementation of a vector dot product ###\n",
    "tic = time.process_time()\n",
    "dot = 0\n",
    "for i in range(len(x1)):\n",
    "    dot += x1[i] * x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Temps de calcul = \" + str(1000*(toc - tic)) + \"ms\\n\")\n",
    "\n",
    "### Classic implementation of an outer product ###\n",
    "tic = time.process_time()\n",
    "outer = np.zeros((len(x1),len(x2))) # creating a matrix of size len(x1)*len(x2) matrix with only zeroes\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i,j] = x1[i] * x2[j]\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Temps de calcul = \" + str(1000*(toc - tic)) + \"ms\\n\")\n",
    "\n",
    "### Classic implementation of element-wise ###\n",
    "tic = time.process_time()\n",
    "mul = np.zeros(len(x1))\n",
    "for i in range(len(x1)):\n",
    "    mul[i] = x1[i] * x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"multiplication élément-wise = \" + str(mul) + \"\\n ----- Temps de calcul = \" + str(1000*(toc - tic)) + \"ms\\n\")\n",
    "\n",
    "### Classic implementation of a general dot product ###\n",
    "W = np.random.rand(3, len(x1)) # Random 3*len(x1) numpy array\n",
    "tic = time.process_time()\n",
    "gdot = np.zeros(W.shape[0])\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(len(x1)):\n",
    "        gdot[i] += W[i,j]*x1[j]\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(gdot) + \"\\n ----- Temps de calcul = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### Numpy implementation of a dot product ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Temps de calcul = \" + str(1000*(toc - tic)) + \"ms\\n\")\n",
    "\n",
    "### Numpy implementation of the outer ###\n",
    "tic = time.process_time()\n",
    "outer = np.outer(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Temps de calcul = \" + str(1000*(toc - tic)) + \"ms\\n\")\n",
    "\n",
    "### Numpy implementation of the 'element-wise ###\n",
    "tic = time.process_time()\n",
    "mul = np.multiply(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Temps de calcul = \" + str(1000*(toc - tic)) + \"ms\\n\")\n",
    "\n",
    "### Numpy implementation of the general dot ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(W,x1)\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(dot) + \"\\n ----- Temps de calcul = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can observe, numpy's implementation is cleaner and more efficient. For vectors and matrices of greater sizes, the difference in terms of computing time becomes greater."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Implement the loss L1 and L2 functions\n",
    "\n",
    "**Exercise**: Implement the numpy version of the loss L1 function. The function abs(x) (absolute value of x) can prove very useful.\n",
    "\n",
    "**Reminder**:\n",
    "- The loss is used to evaluate the performance of your model. The greater the loss, the further apart your predictions ($ \\hat{y} $) are from the real values ($y$). In deep learning, you will use the optimizer algorithms and the gradient descent in order to train your model and minize cost.\n",
    "- L1 is defined as follows:\n",
    "\n",
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1_function(y_predicted, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    y_predicted -- vector of size m containing your predicted values\n",
    "    y -- vector of size m containing the real values\n",
    "    \n",
    "    Returns:\n",
    "    L1 -- loss L1 function value\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Start of code ### (≈ 1 line of code)\n",
    "    L1 = None\n",
    "    ### End of code ###\n",
    "    \n",
    "    return L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = np.array([.8, 0.3, 0.2, .6, .2])\n",
    "y = np.array([1, 1, 0, 1, 0])\n",
    "print(\"L1 = \" + str(L1_function(y_predicted,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected result**:\n",
    "L1 = 1.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implement the vectorised version of the loss L2 function. There are many ways to do it. You might find the np.dot() function useful. As a reminder, if $x = [x_1, x_2, ..., x_n]$, alors `np.dot(x,x)` = $\\sum_{j=0}^n x_j^{2}$.\n",
    "\n",
    "- The loss L2 function is defined as follows: $$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_function(y_predicted, y):\n",
    "    ### Start of code ### (≈ 1 line of code)\n",
    "    L2 = None\n",
    "    ### End of code ###\n",
    "    \n",
    "    return L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = np.array([.8, 0.3, 0.2, .6, .2])\n",
    "y = np.array([1, 1, 0, 1, 0])\n",
    "print(\"L2 = \" + str(L2_function(y_predicted,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected result**: \n",
    "L2 = 0.77"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you have finished this workshop ! We hope that his little exercise allowed you to familiarize yourself with Python and numpy, which constitutes a good base for future workshops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**What you must remember:**\n",
    "- Vectorization is very important in deep learning. It allows you to have a better calculation performance and a more readable code\n",
    "- The L1 and L2 functions\n",
    "- You can now see other numpy functions like np.sum, np.dot, np.multiply, np.maximum, etc..."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XHpfv",
   "launcher_item_id": "Zh0CU"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
